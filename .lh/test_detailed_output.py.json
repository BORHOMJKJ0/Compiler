{
    "sourceFile": "test_detailed_output.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1769012773811,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1769012804555,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,15 +3,22 @@\n Detailed test to show AST JSON output for each SQL file.\n \"\"\"\n \n import json\n+import sys\n from pathlib import Path\n from antlr4 import InputStream, CommonTokenStream\n from BaseLexer import BaseLexer as MyBaseLexer\n from SQLParser import SQLParser as MyParser\n from builders.ast_builder import AstBuilder\n from dataclasses import asdict, is_dataclass\n \n+# Fix encoding on Windows\n+if sys.stdout.encoding != 'utf-8':\n+    import io\n+    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n+\n+\n def to_dict(obj):\n     \"\"\"Convert dataclass objects to dictionaries recursively.\"\"\"\n     if is_dataclass(obj):\n         return {k: to_dict(v) for k, v in asdict(obj).items()}\n@@ -19,39 +26,40 @@\n         return [to_dict(item) for item in obj]\n     else:\n         return obj\n \n+\n def test_sql_file_detailed(filename):\n     \"\"\"Test a single SQL file and show detailed JSON output.\"\"\"\n     filepath = Path(filename)\n     if not filepath.exists():\n         print(f\"File not found: {filename}\")\n         return\n-    \n+\n     print(f\"\\n{'='*80}\")\n     print(f\"TESTING: {filename}\")\n     print('='*80)\n-    \n+\n     with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n         sql_content = f.read()\n-    \n+\n     print(f\"SQL Content ({len(sql_content)} chars):\")\n     print(sql_content[:300] + (\"...\" if len(sql_content) > 300 else \"\"))\n-    \n+\n     # Parse\n     lexer = MyBaseLexer(InputStream(sql_content))\n     token_stream = CommonTokenStream(lexer)\n     token_stream.fill()\n     parser = MyParser(token_stream)\n     tree = parser.sqlScript()\n-    \n+\n     # Build AST\n     builder = AstBuilder()\n     ast = builder.visit(tree)\n-    \n+\n     # Convert to dict\n     ast_dict = to_dict(ast)\n-    \n+\n     # Show statistics\n     if isinstance(ast, list):\n         print(f\"\\n✓ Parsed {len(ast)} statement(s)\")\n         print(f\"\\nFirst 2 statements (JSON):\")\n@@ -65,8 +73,9 @@\n         print(f\"\\nStatement (JSON):\")\n         print(json.dumps(ast_dict, indent=2, default=str)[:1500])\n         print(\"...\")\n \n+\n # Test each file\n test_sql_file_detailed('train.sql')\n test_sql_file_detailed('train2.sql')\n test_sql_file_detailed('testing.sql')\n"
                }
            ],
            "date": 1769012773811,
            "name": "Commit-0",
            "content": "#!/usr/bin/env python3\n\"\"\"\nDetailed test to show AST JSON output for each SQL file.\n\"\"\"\n\nimport json\nfrom pathlib import Path\nfrom antlr4 import InputStream, CommonTokenStream\nfrom BaseLexer import BaseLexer as MyBaseLexer\nfrom SQLParser import SQLParser as MyParser\nfrom builders.ast_builder import AstBuilder\nfrom dataclasses import asdict, is_dataclass\n\ndef to_dict(obj):\n    \"\"\"Convert dataclass objects to dictionaries recursively.\"\"\"\n    if is_dataclass(obj):\n        return {k: to_dict(v) for k, v in asdict(obj).items()}\n    elif isinstance(obj, list):\n        return [to_dict(item) for item in obj]\n    else:\n        return obj\n\ndef test_sql_file_detailed(filename):\n    \"\"\"Test a single SQL file and show detailed JSON output.\"\"\"\n    filepath = Path(filename)\n    if not filepath.exists():\n        print(f\"File not found: {filename}\")\n        return\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"TESTING: {filename}\")\n    print('='*80)\n    \n    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n        sql_content = f.read()\n    \n    print(f\"SQL Content ({len(sql_content)} chars):\")\n    print(sql_content[:300] + (\"...\" if len(sql_content) > 300 else \"\"))\n    \n    # Parse\n    lexer = MyBaseLexer(InputStream(sql_content))\n    token_stream = CommonTokenStream(lexer)\n    token_stream.fill()\n    parser = MyParser(token_stream)\n    tree = parser.sqlScript()\n    \n    # Build AST\n    builder = AstBuilder()\n    ast = builder.visit(tree)\n    \n    # Convert to dict\n    ast_dict = to_dict(ast)\n    \n    # Show statistics\n    if isinstance(ast, list):\n        print(f\"\\n✓ Parsed {len(ast)} statement(s)\")\n        print(f\"\\nFirst 2 statements (JSON):\")\n        for i, stmt in enumerate(ast[:2]):\n            stmt_dict = to_dict(stmt)\n            print(f\"\\nStatement {i+1}:\")\n            print(json.dumps(stmt_dict, indent=2, default=str)[:1000])\n            print(\"...\")\n    else:\n        print(f\"\\n✓ Parsed 1 statement\")\n        print(f\"\\nStatement (JSON):\")\n        print(json.dumps(ast_dict, indent=2, default=str)[:1500])\n        print(\"...\")\n\n# Test each file\ntest_sql_file_detailed('train.sql')\ntest_sql_file_detailed('train2.sql')\ntest_sql_file_detailed('testing.sql')\ntest_sql_file_detailed('full_sql_test.sql')\n"
        }
    ]
}