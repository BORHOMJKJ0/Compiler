{
    "sourceFile": "testLexer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 6,
            "patches": [
                {
                    "date": 1768914525077,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1768914532060,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -59,9 +59,8 @@\n         return\r\n \r\n     logger.info(f\"Reading file: {filename}\")\r\n \r\n-    # Lexical Analysis\r\n     base_lexer = BaseLexer(input_text)\r\n     tokens = base_lexer.get_token_list()\r\n \r\n     if not tokens:\r\n"
                },
                {
                    "date": 1768914537716,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -89,9 +89,8 @@\n         for error in all_errors:\r\n             print(f\"{RED}✗ {error}{RESET}\")\r\n         print()\r\n \r\n-    # Print Warnings\r\n     if all_warnings:\r\n         print(f\"\\n{YELLOW}{'=' * 80}\")\r\n         print(f\"{'WARNINGS':^80}\")\r\n         print(f\"{'=' * 80}{RESET}\\n\")\r\n"
                },
                {
                    "date": 1768914544946,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,164 @@\n+from antlr4 import *\r\n+from lexers.base_lexer import BaseLexer\r\n+from lexers.token_classifier import TokenClassifier\r\n+from semantic.semantic_analyzer import SemanticAnalyzer\r\n+from utils.logger import Logger\r\n+\r\n+RESET = \"\\033[0m\"\r\n+RED = \"\\033[91m\"\r\n+GREEN = \"\\033[92m\"\r\n+YELLOW = \"\\033[93m\"\r\n+BLUE = \"\\033[94m\"\r\n+MAGENTA = \"\\033[95m\"\r\n+CYAN = \"\\033[96m\"\r\n+WHITE = \"\\033[97m\"\r\n+ORANGE = \"\\033[38;5;214m\"\r\n+GRAY = \"\\033[90m\"\r\n+PURPLE = \"\\033[38;5;141m\"\r\n+LIME = \"\\033[38;5;118m\"\r\n+\r\n+\r\n+def colorize(ttype, text):\r\n+    if ttype == \"NUMBER\":\r\n+        return GREEN + text + RESET\r\n+    if ttype == \"HEX_STRING\":\r\n+        return LIME + text + RESET\r\n+    if ttype == \"BIT_STRING\":\r\n+        return PURPLE + text + RESET\r\n+    if ttype == \"STRING_SINGLE\" or ttype == \"STRING_DOUBLE\":\r\n+        return MAGENTA + text + RESET\r\n+    if ttype == \"BRACKET_IDENTIFIER\":\r\n+        return CYAN + text + RESET\r\n+    if ttype == \"IDENTIFIER\":\r\n+        return BLUE + text + RESET\r\n+    if ttype == \"KEYWORD\":\r\n+        return YELLOW + text + RESET\r\n+    if ttype == \"OPERATOR\":\r\n+        return RED + text + RESET\r\n+    if ttype == \"VARIABLE\":\r\n+        return ORANGE + text + RESET\r\n+    if ttype == \"PUNCTUATION\":\r\n+        return WHITE + text + RESET\r\n+    if ttype == \"LINE_COMMENT\" or ttype == \"BLOCK_COMMENT_START\":\r\n+        return GRAY + text + RESET\r\n+    return WHITE + text + RESET\r\n+\r\n+\r\n+def main():\r\n+    filename = \"sqlInput.txt\"\r\n+    logger = Logger(\"TestLexer\")\r\n+\r\n+    try:\r\n+        with open(filename, 'r', encoding='utf-8') as f:\r\n+            input_text = f.read()\r\n+    except FileNotFoundError:\r\n+        print(f\"{RED}Error: File {filename} not found!{RESET}\")\r\n+        return\r\n+    except Exception as e:\r\n+        print(f\"{RED}Error reading file: {e}{RESET}\")\r\n+        return\r\n+\r\n+    logger.info(f\"Reading file: {filename}\")\r\n+\r\n+    base_lexer = BaseLexer(input_text)\r\n+    tokens = base_lexer.get_token_list()\r\n+\r\n+    if not tokens:\r\n+        print(f\"{YELLOW}Warning: No tokens found!{RESET}\")\r\n+        return\r\n+\r\n+    logger.info(f\"Tokens generated: {len(tokens)}\")\r\n+\r\n+    classifier = TokenClassifier()\r\n+    classifier.validate_syntax(tokens, input_text)\r\n+\r\n+    print(f\"\\n{CYAN}{'=' * 80}\")\r\n+    print(f\"{'PERFORMING SEMANTIC ANALYSIS':^80}\")\r\n+    print(f\"{'=' * 80}{RESET}\\n\")\r\n+\r\n+    semantic = SemanticAnalyzer()\r\n+    sem_errors, sem_warnings = semantic.analyze_tokens(tokens)\r\n+\r\n+    all_errors = classifier.errors + [str(e) for e in sem_errors]\r\n+    all_warnings = classifier.warnings + [str(w) for w in sem_warnings]\r\n+\r\n+    if all_errors:\r\n+        print(f\"\\n{RED}{'=' * 80}\")\r\n+        print(f\"{'ERRORS':^80}\")\r\n+        print(f\"{'=' * 80}{RESET}\\n\")\r\n+        for error in all_errors:\r\n+            print(f\"{RED}✗ {error}{RESET}\")\r\n+        print()\r\n+\r\n+    if all_warnings:\r\n+        print(f\"\\n{YELLOW}{'=' * 80}\")\r\n+        print(f\"{'WARNINGS':^80}\")\r\n+        print(f\"{'=' * 80}{RESET}\\n\")\r\n+        for warning in all_warnings:\r\n+            print(f\"{YELLOW}⚠ {warning}{RESET}\")\r\n+        print()\r\n+\r\n+    with open(\"sqlInput.txt\", \"r\") as f:\r\n+        input_text = f.read()\r\n+    lexer = BaseLexer(input_text)\r\n+    raw_tokens = lexer.tokenize()\r\n+\r\n+    line_width = max(len(str(t.line)) for t in raw_tokens) + 2\r\n+    col_width = max(len(str(t.column)) for t in raw_tokens) + 2\r\n+    type_width = max(len(str(lexer.get_token_type_name(t.type) or \"UNKNOWN\")) for t in raw_tokens) + 2\r\n+    val_width = min(max(len(str(t.text)) for t in raw_tokens) + 2, 60)\r\n+\r\n+    print(f\"\\n{CYAN}{'=' * 80}\")\r\n+    print(f\"{'TOKEN TABLE':^80}\")\r\n+    print(f\"{'=' * 80}{RESET}\\n\")\r\n+\r\n+    print(CYAN + \"┌\" + \"─\" * line_width + \"┬\" + \"─\" * col_width +\r\n+          \"┬\" + \"─\" * type_width + \"┬\" + \"─\" * val_width + \"┐\" + RESET)\r\n+    print(f\"│ {'Line'.ljust(line_width - 1)} │ {'Column'.ljust(col_width - 1)} │ \"\r\n+          f\"{'Token Type'.ljust(type_width - 1)} │ {'Value'.ljust(val_width - 1)} │\")\r\n+    print(CYAN + \"├\" + \"─\" * line_width + \"┼\" + \"─\" * col_width +\r\n+          \"┼\" + \"─\" * type_width + \"┼\" + \"─\" * val_width + \"┤\" + RESET)\r\n+\r\n+    for token in raw_tokens:\r\n+        ttype = lexer.get_token_type_name(token.type) or \"UNKNOWN\"\r\n+        display_value = token.text if len(token.text) <= 50 else token.text[:47] + \"...\"\r\n+        colored_value = colorize(ttype, display_value)\r\n+\r\n+        ansi_length = len(colored_value) - len(display_value)\r\n+        adjusted_width = val_width + ansi_length - 1\r\n+\r\n+        print(f\"│ {str(token.line).ljust(line_width - 1)} │ \"\r\n+              f\"{str(token.column).ljust(col_width - 1)} │ \"\r\n+              f\"{ttype.ljust(type_width - 1)} │ \"\r\n+              f\"{colored_value.ljust(adjusted_width)} │\")\r\n+\r\n+    print(CYAN + \"└\" + \"─\" * line_width + \"┴\" + \"─\" * col_width +\r\n+          \"┴\" + \"─\" * type_width + \"┴\" + \"─\" * val_width + \"┘\" + RESET)\r\n+\r\n+    print(f\"\\n{CYAN}{'=' * 80}{RESET}\")\r\n+    token_types = {}\r\n+    for token in raw_tokens:\r\n+        ttype = lexer.get_token_type_name(token.type) or \"UNKNOWN\"\r\n+        token_types[ttype] = token_types.get(ttype, 0) + 1\r\n+\r\n+    print(f\"\\n{GREEN}Total Tokens: {len(raw_tokens)}{RESET}\")\r\n+    print(f\"{RED}Errors: {len(all_errors)}{RESET}\")\r\n+    print(f\"{YELLOW}Warnings: {len(all_warnings)}{RESET}\\n\")\r\n+\r\n+    print(f\"{CYAN}Token Type Distribution:{RESET}\")\r\n+    for ttype, count in sorted(token_types.items(), key=lambda x: -x[1]):\r\n+        print(f\"  {colorize(ttype, ttype)}: {count}\")\r\n+\r\n+    # Semantic Report\r\n+    print(f\"\\n{CYAN}{'=' * 80}\")\r\n+    print(semantic.generate_report())\r\n+\r\n+    # Final Status\r\n+    if not all_errors:\r\n+        print(f\"\\n{GREEN}✓ Analysis completed successfully without errors!{RESET}\\n\")\r\n+    else:\r\n+        print(f\"\\n{RED}✗ Analysis failed! Please fix the errors above.{RESET}\\n\")\r\n+\r\n+\r\n+if __name__ == \"__main__\":\r\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768914551427,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -148,9 +148,8 @@\n     print(f\"{CYAN}Token Type Distribution:{RESET}\")\r\n     for ttype, count in sorted(token_types.items(), key=lambda x: -x[1]):\r\n         print(f\"  {colorize(ttype, ttype)}: {count}\")\r\n \r\n-    # Semantic Report\r\n     print(f\"\\n{CYAN}{'=' * 80}\")\r\n     print(semantic.generate_report())\r\n \r\n     # Final Status\r\n"
                },
                {
                    "date": 1768914560933,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,162 @@\n+from antlr4 import *\r\n+from lexers.base_lexer import BaseLexer\r\n+from lexers.token_classifier import TokenClassifier\r\n+from semantic.semantic_analyzer import SemanticAnalyzer\r\n+from utils.logger import Logger\r\n+\r\n+RESET = \"\\033[0m\"\r\n+RED = \"\\033[91m\"\r\n+GREEN = \"\\033[92m\"\r\n+YELLOW = \"\\033[93m\"\r\n+BLUE = \"\\033[94m\"\r\n+MAGENTA = \"\\033[95m\"\r\n+CYAN = \"\\033[96m\"\r\n+WHITE = \"\\033[97m\"\r\n+ORANGE = \"\\033[38;5;214m\"\r\n+GRAY = \"\\033[90m\"\r\n+PURPLE = \"\\033[38;5;141m\"\r\n+LIME = \"\\033[38;5;118m\"\r\n+\r\n+\r\n+def colorize(ttype, text):\r\n+    if ttype == \"NUMBER\":\r\n+        return GREEN + text + RESET\r\n+    if ttype == \"HEX_STRING\":\r\n+        return LIME + text + RESET\r\n+    if ttype == \"BIT_STRING\":\r\n+        return PURPLE + text + RESET\r\n+    if ttype == \"STRING_SINGLE\" or ttype == \"STRING_DOUBLE\":\r\n+        return MAGENTA + text + RESET\r\n+    if ttype == \"BRACKET_IDENTIFIER\":\r\n+        return CYAN + text + RESET\r\n+    if ttype == \"IDENTIFIER\":\r\n+        return BLUE + text + RESET\r\n+    if ttype == \"KEYWORD\":\r\n+        return YELLOW + text + RESET\r\n+    if ttype == \"OPERATOR\":\r\n+        return RED + text + RESET\r\n+    if ttype == \"VARIABLE\":\r\n+        return ORANGE + text + RESET\r\n+    if ttype == \"PUNCTUATION\":\r\n+        return WHITE + text + RESET\r\n+    if ttype == \"LINE_COMMENT\" or ttype == \"BLOCK_COMMENT_START\":\r\n+        return GRAY + text + RESET\r\n+    return WHITE + text + RESET\r\n+\r\n+\r\n+def main():\r\n+    filename = \"sqlInput.txt\"\r\n+    logger = Logger(\"TestLexer\")\r\n+\r\n+    try:\r\n+        with open(filename, 'r', encoding='utf-8') as f:\r\n+            input_text = f.read()\r\n+    except FileNotFoundError:\r\n+        print(f\"{RED}Error: File {filename} not found!{RESET}\")\r\n+        return\r\n+    except Exception as e:\r\n+        print(f\"{RED}Error reading file: {e}{RESET}\")\r\n+        return\r\n+\r\n+    logger.info(f\"Reading file: {filename}\")\r\n+\r\n+    base_lexer = BaseLexer(input_text)\r\n+    tokens = base_lexer.get_token_list()\r\n+\r\n+    if not tokens:\r\n+        print(f\"{YELLOW}Warning: No tokens found!{RESET}\")\r\n+        return\r\n+\r\n+    logger.info(f\"Tokens generated: {len(tokens)}\")\r\n+\r\n+    classifier = TokenClassifier()\r\n+    classifier.validate_syntax(tokens, input_text)\r\n+\r\n+    print(f\"\\n{CYAN}{'=' * 80}\")\r\n+    print(f\"{'PERFORMING SEMANTIC ANALYSIS':^80}\")\r\n+    print(f\"{'=' * 80}{RESET}\\n\")\r\n+\r\n+    semantic = SemanticAnalyzer()\r\n+    sem_errors, sem_warnings = semantic.analyze_tokens(tokens)\r\n+\r\n+    all_errors = classifier.errors + [str(e) for e in sem_errors]\r\n+    all_warnings = classifier.warnings + [str(w) for w in sem_warnings]\r\n+\r\n+    if all_errors:\r\n+        print(f\"\\n{RED}{'=' * 80}\")\r\n+        print(f\"{'ERRORS':^80}\")\r\n+        print(f\"{'=' * 80}{RESET}\\n\")\r\n+        for error in all_errors:\r\n+            print(f\"{RED}✗ {error}{RESET}\")\r\n+        print()\r\n+\r\n+    if all_warnings:\r\n+        print(f\"\\n{YELLOW}{'=' * 80}\")\r\n+        print(f\"{'WARNINGS':^80}\")\r\n+        print(f\"{'=' * 80}{RESET}\\n\")\r\n+        for warning in all_warnings:\r\n+            print(f\"{YELLOW} {warning}{RESET}\")\r\n+        print()\r\n+\r\n+    with open(\"sqlInput.txt\", \"r\") as f:\r\n+        input_text = f.read()\r\n+    lexer = BaseLexer(input_text)\r\n+    raw_tokens = lexer.tokenize()\r\n+\r\n+    line_width = max(len(str(t.line)) for t in raw_tokens) + 2\r\n+    col_width = max(len(str(t.column)) for t in raw_tokens) + 2\r\n+    type_width = max(len(str(lexer.get_token_type_name(t.type) or \"UNKNOWN\")) for t in raw_tokens) + 2\r\n+    val_width = min(max(len(str(t.text)) for t in raw_tokens) + 2, 60)\r\n+\r\n+    print(f\"\\n{CYAN}{'=' * 80}\")\r\n+    print(f\"{'TOKEN TABLE':^80}\")\r\n+    print(f\"{'=' * 80}{RESET}\\n\")\r\n+\r\n+    print(CYAN + \"┌\" + \"─\" * line_width + \"┬\" + \"─\" * col_width +\r\n+          \"┬\" + \"─\" * type_width + \"┬\" + \"─\" * val_width + \"┐\" + RESET)\r\n+    print(f\"│ {'Line'.ljust(line_width - 1)} │ {'Column'.ljust(col_width - 1)} │ \"\r\n+          f\"{'Token Type'.ljust(type_width - 1)} │ {'Value'.ljust(val_width - 1)} │\")\r\n+    print(CYAN + \"├\" + \"─\" * line_width + \"┼\" + \"─\" * col_width +\r\n+          \"┼\" + \"─\" * type_width + \"┼\" + \"─\" * val_width + \"┤\" + RESET)\r\n+\r\n+    for token in raw_tokens:\r\n+        ttype = lexer.get_token_type_name(token.type) or \"UNKNOWN\"\r\n+        display_value = token.text if len(token.text) <= 50 else token.text[:47] + \"...\"\r\n+        colored_value = colorize(ttype, display_value)\r\n+\r\n+        ansi_length = len(colored_value) - len(display_value)\r\n+        adjusted_width = val_width + ansi_length - 1\r\n+\r\n+        print(f\"│ {str(token.line).ljust(line_width - 1)} │ \"\r\n+              f\"{str(token.column).ljust(col_width - 1)} │ \"\r\n+              f\"{ttype.ljust(type_width - 1)} │ \"\r\n+              f\"{colored_value.ljust(adjusted_width)} │\")\r\n+\r\n+    print(CYAN + \"└\" + \"─\" * line_width + \"┴\" + \"─\" * col_width +\r\n+          \"┴\" + \"─\" * type_width + \"┴\" + \"─\" * val_width + \"┘\" + RESET)\r\n+\r\n+    print(f\"\\n{CYAN}{'=' * 80}{RESET}\")\r\n+    token_types = {}\r\n+    for token in raw_tokens:\r\n+        ttype = lexer.get_token_type_name(token.type) or \"UNKNOWN\"\r\n+        token_types[ttype] = token_types.get(ttype, 0) + 1\r\n+\r\n+    print(f\"\\n{GREEN}Total Tokens: {len(raw_tokens)}{RESET}\")\r\n+    print(f\"{RED}Errors: {len(all_errors)}{RESET}\")\r\n+    print(f\"{YELLOW}Warnings: {len(all_warnings)}{RESET}\\n\")\r\n+\r\n+    print(f\"{CYAN}Token Type Distribution:{RESET}\")\r\n+    for ttype, count in sorted(token_types.items(), key=lambda x: -x[1]):\r\n+        print(f\"  {colorize(ttype, ttype)}: {count}\")\r\n+\r\n+    print(f\"\\n{CYAN}{'=' * 80}\")\r\n+    print(semantic.generate_report())\r\n+\r\n+    if not all_errors:\r\n+        print(f\"\\n{GREEN}Analysis completed successfully without errors!{RESET}\\n\")\r\n+    else:\r\n+        print(f\"\\n{RED}Analysis failed! Please fix the errors above.{RESET}\\n\")\r\n+\r\n+\r\n+if __name__ == \"__main__\":\r\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1768914574665,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -104,9 +104,10 @@\n     raw_tokens = lexer.tokenize()\r\n \r\n     line_width = max(len(str(t.line)) for t in raw_tokens) + 2\r\n     col_width = max(len(str(t.column)) for t in raw_tokens) + 2\r\n-    type_width = max(len(str(lexer.get_token_type_name(t.type) or \"UNKNOWN\")) for t in raw_tokens) + 2\r\n+    type_width = max(len(str(lexer.get_token_type_name(\r\n+        t.type) or \"UNKNOWN\")) for t in raw_tokens) + 2\r\n     val_width = min(max(len(str(t.text)) for t in raw_tokens) + 2, 60)\r\n \r\n     print(f\"\\n{CYAN}{'=' * 80}\")\r\n     print(f\"{'TOKEN TABLE':^80}\")\r\n@@ -120,9 +121,10 @@\n           \"┼\" + \"─\" * type_width + \"┼\" + \"─\" * val_width + \"┤\" + RESET)\r\n \r\n     for token in raw_tokens:\r\n         ttype = lexer.get_token_type_name(token.type) or \"UNKNOWN\"\r\n-        display_value = token.text if len(token.text) <= 50 else token.text[:47] + \"...\"\r\n+        display_value = token.text if len(\r\n+            token.text) <= 50 else token.text[:47] + \"...\"\r\n         colored_value = colorize(ttype, display_value)\r\n \r\n         ansi_length = len(colored_value) - len(display_value)\r\n         adjusted_width = val_width + ansi_length - 1\r\n@@ -158,5 +160,5 @@\n         print(f\"\\n{RED}Analysis failed! Please fix the errors above.{RESET}\\n\")\r\n \r\n \r\n if __name__ == \"__main__\":\r\n-    main()\n\\ No newline at end of file\n+    main()\r\n"
                }
            ],
            "date": 1768914525077,
            "name": "Commit-0",
            "content": "from antlr4 import *\r\nfrom lexers.base_lexer import BaseLexer\r\nfrom lexers.token_classifier import TokenClassifier\r\nfrom semantic.semantic_analyzer import SemanticAnalyzer\r\nfrom utils.logger import Logger\r\n\r\n# ANSI Colors\r\nRESET = \"\\033[0m\"\r\nRED = \"\\033[91m\"\r\nGREEN = \"\\033[92m\"\r\nYELLOW = \"\\033[93m\"\r\nBLUE = \"\\033[94m\"\r\nMAGENTA = \"\\033[95m\"\r\nCYAN = \"\\033[96m\"\r\nWHITE = \"\\033[97m\"\r\nORANGE = \"\\033[38;5;214m\"\r\nGRAY = \"\\033[90m\"\r\nPURPLE = \"\\033[38;5;141m\"\r\nLIME = \"\\033[38;5;118m\"\r\n\r\n\r\ndef colorize(ttype, text):\r\n    if ttype == \"NUMBER\":\r\n        return GREEN + text + RESET\r\n    if ttype == \"HEX_STRING\":\r\n        return LIME + text + RESET\r\n    if ttype == \"BIT_STRING\":\r\n        return PURPLE + text + RESET\r\n    if ttype == \"STRING_SINGLE\" or ttype == \"STRING_DOUBLE\":\r\n        return MAGENTA + text + RESET\r\n    if ttype == \"BRACKET_IDENTIFIER\":\r\n        return CYAN + text + RESET\r\n    if ttype == \"IDENTIFIER\":\r\n        return BLUE + text + RESET\r\n    if ttype == \"KEYWORD\":\r\n        return YELLOW + text + RESET\r\n    if ttype == \"OPERATOR\":\r\n        return RED + text + RESET\r\n    if ttype == \"VARIABLE\":\r\n        return ORANGE + text + RESET\r\n    if ttype == \"PUNCTUATION\":\r\n        return WHITE + text + RESET\r\n    if ttype == \"LINE_COMMENT\" or ttype == \"BLOCK_COMMENT_START\":\r\n        return GRAY + text + RESET\r\n    return WHITE + text + RESET\r\n\r\n\r\ndef main():\r\n    filename = \"sqlInput.txt\"\r\n    logger = Logger(\"TestLexer\")\r\n\r\n    try:\r\n        with open(filename, 'r', encoding='utf-8') as f:\r\n            input_text = f.read()\r\n    except FileNotFoundError:\r\n        print(f\"{RED}Error: File {filename} not found!{RESET}\")\r\n        return\r\n    except Exception as e:\r\n        print(f\"{RED}Error reading file: {e}{RESET}\")\r\n        return\r\n\r\n    logger.info(f\"Reading file: {filename}\")\r\n\r\n    # Lexical Analysis\r\n    base_lexer = BaseLexer(input_text)\r\n    tokens = base_lexer.get_token_list()\r\n\r\n    if not tokens:\r\n        print(f\"{YELLOW}Warning: No tokens found!{RESET}\")\r\n        return\r\n\r\n    logger.info(f\"Tokens generated: {len(tokens)}\")\r\n\r\n    # Token Classification and Validation\r\n    classifier = TokenClassifier()\r\n    classifier.validate_syntax(tokens, input_text)\r\n\r\n    # Semantic Analysis\r\n    print(f\"\\n{CYAN}{'=' * 80}\")\r\n    print(f\"{'PERFORMING SEMANTIC ANALYSIS':^80}\")\r\n    print(f\"{'=' * 80}{RESET}\\n\")\r\n\r\n    semantic = SemanticAnalyzer()\r\n    sem_errors, sem_warnings = semantic.analyze_tokens(tokens)\r\n\r\n    all_errors = classifier.errors + [str(e) for e in sem_errors]\r\n    all_warnings = classifier.warnings + [str(w) for w in sem_warnings]\r\n\r\n    # Print Errors\r\n    if all_errors:\r\n        print(f\"\\n{RED}{'=' * 80}\")\r\n        print(f\"{'ERRORS':^80}\")\r\n        print(f\"{'=' * 80}{RESET}\\n\")\r\n        for error in all_errors:\r\n            print(f\"{RED}✗ {error}{RESET}\")\r\n        print()\r\n\r\n    # Print Warnings\r\n    if all_warnings:\r\n        print(f\"\\n{YELLOW}{'=' * 80}\")\r\n        print(f\"{'WARNINGS':^80}\")\r\n        print(f\"{'=' * 80}{RESET}\\n\")\r\n        for warning in all_warnings:\r\n            print(f\"{YELLOW}⚠ {warning}{RESET}\")\r\n        print()\r\n\r\n    # Calculate column widths\r\n    with open(\"sqlInput.txt\", \"r\") as f:\r\n        input_text = f.read()\r\n    lexer = BaseLexer(input_text)\r\n    raw_tokens = lexer.tokenize()\r\n\r\n    line_width = max(len(str(t.line)) for t in raw_tokens) + 2\r\n    col_width = max(len(str(t.column)) for t in raw_tokens) + 2\r\n    type_width = max(len(str(lexer.get_token_type_name(t.type) or \"UNKNOWN\")) for t in raw_tokens) + 2\r\n    val_width = min(max(len(str(t.text)) for t in raw_tokens) + 2, 60)\r\n\r\n    # Print Token Table\r\n    print(f\"\\n{CYAN}{'=' * 80}\")\r\n    print(f\"{'TOKEN TABLE':^80}\")\r\n    print(f\"{'=' * 80}{RESET}\\n\")\r\n\r\n    print(CYAN + \"┌\" + \"─\" * line_width + \"┬\" + \"─\" * col_width +\r\n          \"┬\" + \"─\" * type_width + \"┬\" + \"─\" * val_width + \"┐\" + RESET)\r\n    print(f\"│ {'Line'.ljust(line_width - 1)} │ {'Column'.ljust(col_width - 1)} │ \"\r\n          f\"{'Token Type'.ljust(type_width - 1)} │ {'Value'.ljust(val_width - 1)} │\")\r\n    print(CYAN + \"├\" + \"─\" * line_width + \"┼\" + \"─\" * col_width +\r\n          \"┼\" + \"─\" * type_width + \"┼\" + \"─\" * val_width + \"┤\" + RESET)\r\n\r\n    for token in raw_tokens:\r\n        ttype = lexer.get_token_type_name(token.type) or \"UNKNOWN\"\r\n        display_value = token.text if len(token.text) <= 50 else token.text[:47] + \"...\"\r\n        colored_value = colorize(ttype, display_value)\r\n\r\n        ansi_length = len(colored_value) - len(display_value)\r\n        adjusted_width = val_width + ansi_length - 1\r\n\r\n        print(f\"│ {str(token.line).ljust(line_width - 1)} │ \"\r\n              f\"{str(token.column).ljust(col_width - 1)} │ \"\r\n              f\"{ttype.ljust(type_width - 1)} │ \"\r\n              f\"{colored_value.ljust(adjusted_width)} │\")\r\n\r\n    print(CYAN + \"└\" + \"─\" * line_width + \"┴\" + \"─\" * col_width +\r\n          \"┴\" + \"─\" * type_width + \"┴\" + \"─\" * val_width + \"┘\" + RESET)\r\n\r\n    # Token Statistics\r\n    print(f\"\\n{CYAN}{'=' * 80}{RESET}\")\r\n    token_types = {}\r\n    for token in raw_tokens:\r\n        ttype = lexer.get_token_type_name(token.type) or \"UNKNOWN\"\r\n        token_types[ttype] = token_types.get(ttype, 0) + 1\r\n\r\n    print(f\"\\n{GREEN}Total Tokens: {len(raw_tokens)}{RESET}\")\r\n    print(f\"{RED}Errors: {len(all_errors)}{RESET}\")\r\n    print(f\"{YELLOW}Warnings: {len(all_warnings)}{RESET}\\n\")\r\n\r\n    print(f\"{CYAN}Token Type Distribution:{RESET}\")\r\n    for ttype, count in sorted(token_types.items(), key=lambda x: -x[1]):\r\n        print(f\"  {colorize(ttype, ttype)}: {count}\")\r\n\r\n    # Semantic Report\r\n    print(f\"\\n{CYAN}{'=' * 80}\")\r\n    print(semantic.generate_report())\r\n\r\n    # Final Status\r\n    if not all_errors:\r\n        print(f\"\\n{GREEN}✓ Analysis completed successfully without errors!{RESET}\\n\")\r\n    else:\r\n        print(f\"\\n{RED}✗ Analysis failed! Please fix the errors above.{RESET}\\n\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()"
        }
    ]
}