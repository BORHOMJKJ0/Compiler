{
    "sourceFile": "testLexer.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 2,
            "patches": [
                {
                    "date": 1764232360757,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1764236912578,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,52 +9,153 @@\n MAGENTA = \"\\033[95m\"\n CYAN = \"\\033[96m\"\n WHITE = \"\\033[97m\"\n ORANGE = \"\\033[38;5;214m\"\n+GRAY = \"\\033[90m\"\n \n \n def colorize(ttype, text):\n     if ttype == \"NUMBER\":\n         return GREEN + text + RESET\n-    if ttype == \"STRING\":\n+    if ttype == \"STRING_SINGLE\" or ttype == \"STRING_DOUBLE\":\n         return MAGENTA + text + RESET\n+    if ttype == \"BRACKET_IDENTIFIER\":\n+        return CYAN + text + RESET\n     if ttype == \"IDENTIFIER\":\n         return BLUE + text + RESET\n     if ttype == \"KEYWORD\":\n         return YELLOW + text + RESET\n     if ttype == \"OPERATOR\":\n         return RED + text + RESET\n     if ttype == \"VARIABLE\":\n         return ORANGE + text + RESET\n+    if ttype == \"PUNCTUATION\":\n+        return WHITE + text + RESET\n+    if ttype == \"INVALID_IDENTIFIER\":\n+        return RED + text + RESET\n+    if ttype == \"LINE_COMMENT\" or ttype == \"BLOCK_COMMENT_START\" or ttype == \"COMMENT_CONTENT\":\n+        return GRAY + text + RESET\n     return WHITE + text + RESET\n \n \n+def check_syntax_errors(tokens, lexer, input_text):\n+    errors = []\n+    warnings = []\n+    \n+    paren_count = 0\n+    \n+    for token in tokens:\n+        ttype = lexer.symbolicNames[token.type] if token.type < len(lexer.symbolicNames) else \"UNKNOWN\"\n+        \n+        if token.text == '(':\n+            paren_count += 1\n+        elif token.text == ')':\n+            paren_count -= 1\n+            if paren_count < 0:\n+                errors.append(f\"Line {token.line}, Column {token.column}: Closing parenthesis ) without opening\")\n+                paren_count = 0\n+        \n+        if ttype == \"INVALID_IDENTIFIER\":\n+            if token.text.startswith('_') and all(c == '_' for c in token.text):\n+                errors.append(f\"Line {token.line}, Column {token.column}: Invalid identifier '{token.text}' - Cannot use only underscores\")\n+            elif token.text[0].isdigit():\n+                errors.append(f\"Line {token.line}, Column {token.column}: Invalid identifier '{token.text}' - Cannot start with digit\")\n+            else:\n+                errors.append(f\"Line {token.line}, Column {token.column}: Invalid identifier '{token.text}'\")\n+        \n+        if token.text.startswith('[') and token.text.endswith(']'):\n+            if \"'\" in token.text:\n+                warnings.append(f\"Line {token.line}, Column {token.column}: Identifier {token.text} contains single quote\")\n+        \n+        if (token.text.startswith(\"'\") and token.text.endswith(\"'\")) or \\\n+           (token.text.startswith('\"') and token.text.endswith('\"')):\n+            if \"''\" in token.text[1:-1] or '\"\"' in token.text[1:-1]:\n+                warnings.append(f\"Line {token.line}, Column {token.column}: String contains escaped quotes\")\n+    \n+    if paren_count > 0:\n+        errors.append(f\"Error: {paren_count} unclosed opening parenthesis\")\n+    \n+    comment_depth = 0\n+    lines = input_text.split('\\n')\n+    for line_num, line in enumerate(lines, 1):\n+        i = 0\n+        while i < len(line) - 1:\n+            if line[i:i+2] == '/*':\n+                comment_depth += 1\n+                i += 2\n+            elif line[i:i+2] == '*/':\n+                comment_depth -= 1\n+                if comment_depth < 0:\n+                    errors.append(f\"Line {line_num}: Closing comment */ without opening /*\")\n+                    comment_depth = 0\n+                i += 2\n+            else:\n+                i += 1\n+    \n+    if comment_depth > 0:\n+        errors.append(f\"Error: {comment_depth} unclosed block comment\")\n+    \n+    return errors, warnings\n+\n+\n def main():\n-    input_stream = FileStream(\"sqlInput.txt\", encoding=\"utf-8\")\n+    try:\n+        input_stream = FileStream(\"testing.sql\", encoding=\"utf-8\")\n+    except FileNotFoundError:\n+        print(f\"{RED}Error: File testing.sql not found!{RESET}\")\n+        return\n+    except Exception as e:\n+        print(f\"{RED}Error reading file: {e}{RESET}\")\n+        return\n+\n     lexer = MyLexer(input_stream)\n     tokens = lexer.getAllTokens()\n \n     if not tokens:\n-        print(\"No tokens found!\")\n+        print(f\"{YELLOW}Warning: No tokens found!{RESET}\")\n         return\n \n+    input_stream.seek(0)\n+    input_text = input_stream.readall()\n+    errors, warnings = check_syntax_errors(tokens, lexer, input_text)\n+\n+    if errors:\n+        print(f\"\\n{RED}{'='*80}\")\n+        print(f\"{'  SYNTAX ERRORS  ':^80}\")\n+        print(f\"{'='*80}{RESET}\\n\")\n+        for error in errors:\n+            print(f\"{RED}✗ {error}{RESET}\")\n+        print()\n+\n+    if warnings:\n+        print(f\"\\n{YELLOW}{'='*80}\")\n+        print(f\"{'  WARNINGS  ':^80}\")\n+        print(f\"{'='*80}{RESET}\\n\")\n+        for warning in warnings:\n+            print(f\"{YELLOW}⚠ {warning}{RESET}\")\n+        print()\n+\n     line_width = max(len(str(t.line)) for t in tokens) + 2\n     col_width = max(len(str(t.column)) for t in tokens) + 2\n-    type_width = max(\n-        len(str(lexer.symbolicNames[t.type] or \"UNKNOWN\")) for t in tokens) + 2\n-    val_width = max(len(str(t.text)) for t in tokens) + 2\n+    type_width = max(len(str(lexer.symbolicNames[t.type] or \"UNKNOWN\")) for t in tokens) + 2\n+    val_width = min(max(len(str(t.text)) for t in tokens) + 2, 60)\n \n+    print(f\"\\n{CYAN}{'='*80}\")\n+    print(f\"{'  TOKEN TABLE  ':^80}\")\n+    print(f\"{'='*80}{RESET}\\n\")\n+    \n     print(CYAN + \"┌\" + \"─\"*(line_width) + \"┬\" + \"─\"*(col_width) +\n           \"┬\" + \"─\"*(type_width) + \"┬\" + \"─\"*(val_width) + \"┐\" + RESET)\n     print(f\"│ {'Line'.ljust(line_width-1)} │ {'Column'.ljust(col_width-1)} │ {'Token Type'.ljust(type_width-1)} │ {'Value'.ljust(val_width-1)} │\")\n     print(CYAN + \"├\" + \"─\"*(line_width) + \"┼\" + \"─\"*(col_width) +\n           \"┼\" + \"─\"*(type_width) + \"┼\" + \"─\"*(val_width) + \"┤\" + RESET)\n \n     for token in tokens:\n         ttype = lexer.symbolicNames[token.type] or \"UNKNOWN\"\n-        colored_value = colorize(ttype, token.text)\n+        display_value = token.text if len(token.text) <= 50 else token.text[:47] + \"...\"\n+        colored_value = colorize(ttype, display_value)\n \n-        ansi_length = len(colored_value) - len(token.text)\n+        ansi_length = len(colored_value) - len(display_value)\n         adjusted_width = val_width + ansi_length - 1\n \n         print(\n             f\"│ {str(token.line).ljust(line_width-1)} │ \"\n@@ -64,9 +165,27 @@\n         )\n \n     print(CYAN + \"└\" + \"─\"*(line_width) + \"┴\" + \"─\"*(col_width) +\n           \"┴\" + \"─\"*(type_width) + \"┴\" + \"─\"*(val_width) + \"┘\" + RESET)\n-    print(f\"\\nTotal Tokens: {len(tokens)}\")\n+    \n+    print(f\"\\n{CYAN}{'='*80}{RESET}\")\n+    token_types = {}\n+    for token in tokens:\n+        ttype = lexer.symbolicNames[token.type] or \"UNKNOWN\"\n+        token_types[ttype] = token_types.get(ttype, 0) + 1\n+    \n+    print(f\"\\n{GREEN}Total Tokens: {len(tokens)}{RESET}\")\n+    print(f\"{RED}Errors: {len(errors)}{RESET}\")\n+    print(f\"{YELLOW}Warnings: {len(warnings)}{RESET}\\n\")\n+    \n+    print(f\"{CYAN}Token Type Distribution:{RESET}\")\n+    for ttype, count in sorted(token_types.items(), key=lambda x: -x[1]):\n+        print(f\"  {colorize(ttype, ttype)}: {count}\")\n+    \n+    if not errors:\n+        print(f\"\\n{GREEN}Analysis completed successfully without syntax errors!{RESET}\\n\")\n+    else:\n+        print(f\"\\n{RED}Analysis failed! Please fix the errors above.{RESET}\\n\")\n \n \n if __name__ == \"__main__\":\n-    main()\n+    main()\n\\ No newline at end of file\n"
                },
                {
                    "date": 1764238244249,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -130,9 +130,9 @@\n         print(f\"\\n{YELLOW}{'='*80}\")\n         print(f\"{'  WARNINGS  ':^80}\")\n         print(f\"{'='*80}{RESET}\\n\")\n         for warning in warnings:\n-            print(f\"{YELLOW}⚠ {warning}{RESET}\")\n+            print(f\"{YELLOW} {warning}{RESET}\")\n         print()\n \n     line_width = max(len(str(t.line)) for t in tokens) + 2\n     col_width = max(len(str(t.column)) for t in tokens) + 2\n"
                }
            ],
            "date": 1764232360757,
            "name": "Commit-0",
            "content": "from antlr4 import *\nfrom MyLexer import MyLexer\n\nRESET = \"\\033[0m\"\nRED = \"\\033[91m\"\nGREEN = \"\\033[92m\"\nYELLOW = \"\\033[93m\"\nBLUE = \"\\033[94m\"\nMAGENTA = \"\\033[95m\"\nCYAN = \"\\033[96m\"\nWHITE = \"\\033[97m\"\nORANGE = \"\\033[38;5;214m\"\n\n\ndef colorize(ttype, text):\n    if ttype == \"NUMBER\":\n        return GREEN + text + RESET\n    if ttype == \"STRING\":\n        return MAGENTA + text + RESET\n    if ttype == \"IDENTIFIER\":\n        return BLUE + text + RESET\n    if ttype == \"KEYWORD\":\n        return YELLOW + text + RESET\n    if ttype == \"OPERATOR\":\n        return RED + text + RESET\n    if ttype == \"VARIABLE\":\n        return ORANGE + text + RESET\n    return WHITE + text + RESET\n\n\ndef main():\n    input_stream = FileStream(\"sqlInput.txt\", encoding=\"utf-8\")\n    lexer = MyLexer(input_stream)\n    tokens = lexer.getAllTokens()\n\n    if not tokens:\n        print(\"No tokens found!\")\n        return\n\n    line_width = max(len(str(t.line)) for t in tokens) + 2\n    col_width = max(len(str(t.column)) for t in tokens) + 2\n    type_width = max(\n        len(str(lexer.symbolicNames[t.type] or \"UNKNOWN\")) for t in tokens) + 2\n    val_width = max(len(str(t.text)) for t in tokens) + 2\n\n    print(CYAN + \"┌\" + \"─\"*(line_width) + \"┬\" + \"─\"*(col_width) +\n          \"┬\" + \"─\"*(type_width) + \"┬\" + \"─\"*(val_width) + \"┐\" + RESET)\n    print(f\"│ {'Line'.ljust(line_width-1)} │ {'Column'.ljust(col_width-1)} │ {'Token Type'.ljust(type_width-1)} │ {'Value'.ljust(val_width-1)} │\")\n    print(CYAN + \"├\" + \"─\"*(line_width) + \"┼\" + \"─\"*(col_width) +\n          \"┼\" + \"─\"*(type_width) + \"┼\" + \"─\"*(val_width) + \"┤\" + RESET)\n\n    for token in tokens:\n        ttype = lexer.symbolicNames[token.type] or \"UNKNOWN\"\n        colored_value = colorize(ttype, token.text)\n\n        ansi_length = len(colored_value) - len(token.text)\n        adjusted_width = val_width + ansi_length - 1\n\n        print(\n            f\"│ {str(token.line).ljust(line_width-1)} │ \"\n            f\"{str(token.column).ljust(col_width-1)} │ \"\n            f\"{ttype.ljust(type_width-1)} │ \"\n            f\"{colored_value.ljust(adjusted_width)} │\"\n        )\n\n    print(CYAN + \"└\" + \"─\"*(line_width) + \"┴\" + \"─\"*(col_width) +\n          \"┴\" + \"─\"*(type_width) + \"┴\" + \"─\"*(val_width) + \"┘\" + RESET)\n    print(f\"\\nTotal Tokens: {len(tokens)}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
    ]
}