{
    "sourceFile": "test_all_files.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1768862990032,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1768863007815,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -378,8 +378,9 @@\n         ('sqlInput.txt', 'Main test file - Complex DDL/DML'),\n         ('testing.sql', 'Same as sqlInput - Duplicate test'),\n         ('train.sql', 'Training queries - Various SQL patterns'),\n         ('train2.sql', 'Advanced training - Constraints & JOINs'),\n+        ('train2.sql', 'Advanced training - Constraints & JOINs'),\n     ]\n \n     # Ø§Ø®ØªØ¨Ø§Ø± ÙƒÙ„ Ù…Ù„Ù\n     print(\"ğŸš€ TESTING ALL SQL FILES\")\n"
                }
            ],
            "date": 1768862990032,
            "name": "Commit-0",
            "content": "\"\"\"\nComprehensive SQL Parser Test Suite\nÙ†Ø¸Ø§Ù… Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª SQL Ù…Ø¹ Ø¹Ø±Ø¶ Ø´Ø¬Ø±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„\n\"\"\"\n\nimport sys\nfrom pathlib import Path\nfrom antlr4 import *\nfrom antlr4.error.ErrorListener import ErrorListener\nfrom datetime import datetime\nimport json\n\n\nclass SQLErrorListener(ErrorListener):\n    \"\"\"Ù…Ø³ØªÙ…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.errors = []\n\n    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):\n        self.errors.append({\n            'line': line,\n            'column': column,\n            'message': msg,\n            'symbol': offendingSymbol.text if offendingSymbol else '<EOF>'\n        })\n\n\nclass ComprehensiveTestSuite:\n    \"\"\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø©\"\"\"\n\n    def __init__(self):\n        # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Lexer Ùˆ Parser\n        try:\n            from BaseLexer import BaseLexer\n            from SQLParser import SQLParser\n            self.BaseLexer = BaseLexer\n            self.SQLParser = SQLParser\n            print(\"âœ… Successfully loaded BaseLexer and SQLParser\\n\")\n        except ImportError as e:\n            print(f\"âŒ Error importing modules: {e}\")\n            print(\"   Make sure you have generated the parser files:\")\n            print(\"   antlr4 -Dlanguage=Python3 BaseLexer.g4\")\n            print(\"   antlr4 -Dlanguage=Python3 SQLParser.g4\")\n            sys.exit(1)\n\n        self.results = {\n            'timestamp': datetime.now().isoformat(),\n            'test_files': [],\n            'summary': {\n                'total_files': 0,\n                'passed_files': 0,\n                'failed_files': 0,\n                'total_errors': 0\n            }\n        }\n\n    def test_file(self, filepath, description=\"\"):\n        \"\"\"Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù„Ù SQL ÙˆØ§Ø­Ø¯\"\"\"\n        filepath = Path(filepath)\n\n        print(\"=\" * 80)\n        print(f\"ğŸ§ª TESTING: {filepath.name}\")\n        if description:\n            print(f\"ğŸ“ Description: {description}\")\n        print(\"=\" * 80)\n\n        if not filepath.exists():\n            print(f\"âŒ File not found: {filepath}\")\n            return None\n\n        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù\n        with open(filepath, 'r', encoding='utf-8') as f:\n            sql_code = f.read()\n\n        # Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù„Ù\n        lines = sql_code.strip().split('\\n')\n        print(f\"\\nğŸ“Š File Statistics:\")\n        print(f\"   Lines: {len(lines)}\")\n        print(f\"   Characters: {len(sql_code)}\")\n        print(f\"   Size: {len(sql_code.encode('utf-8'))} bytes\")\n\n        # Parsing\n        result = self._parse(sql_code, filepath.name)\n\n        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n        print(f\"\\n{'=' * 80}\")\n        if result['success']:\n            print(f\"âœ… SUCCESS - File parsed successfully!\")\n            print(f\"   â±ï¸  Parsing Time: {result['parsing_time']:.2f}ms\")\n            print(f\"   ğŸ“ Tree Depth: {result['tree_depth']}\")\n            print(f\"   ğŸŒ³ Node Count: {result['node_count']}\")\n            print(f\"   ğŸ¯ Token Count: {result['token_count']}\")\n            \n            # Ø¹Ø±Ø¶ Ø´Ø¬Ø±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø³Ø·\n            print(f\"\\nğŸŒ³ PARSE TREE PREVIEW:\")\n            print(\"-\" * 40)\n            self._print_tree_preview(result['tree'], self.SQLParser.ruleNames)\n            print(\"-\" * 40)\n            \n        else:\n            print(f\"âŒ FAILED - {len(result['errors'])} error(s) found\")\n            print(f\"\\nğŸ“‹ Errors:\")\n            for i, err in enumerate(result['errors'][:5], 1):  # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 Ø£Ø®Ø·Ø§Ø¡\n                print(f\"\\n   Error {i}:\")\n                print(f\"      ğŸ“ Location: Line {err['line']}, Column {err['column']}\")\n                print(f\"      ğŸ’¬ Message: {err['message'][:100]}...\")\n                print(f\"      ğŸ”¤ Near: '{err['symbol'][:30]}'\")\n\n            if len(result['errors']) > 5:\n                print(f\"\\n   ... and {len(result['errors']) - 5} more errors\")\n\n        print(f\"{'=' * 80}\\n\")\n\n        # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© (Ø¨Ø¯ÙˆÙ† ÙƒØ§Ø¦Ù† Ø§Ù„Ø´Ø¬Ø±Ø© Ù„Ø£Ù†Ù‡ ØºÙŠØ± Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØ­ÙˆÙŠÙ„ Ù„Ù€ JSON Ø¨Ø³Ù‡ÙˆÙ„Ø©)\n        save_result = result.copy()\n        del save_result['tree']\n        self.results['test_files'].append(save_result)\n        self.results['summary']['total_files'] += 1\n\n        if result['success']:\n            self.results['summary']['passed_files'] += 1\n        else:\n            self.results['summary']['failed_files'] += 1\n            self.results['summary']['total_errors'] += len(result['errors'])\n\n        return result\n\n    def _parse(self, sql_code, filename):\n        \"\"\"Ø¯Ø§Ù„Ø© Ø¯Ø§Ø®Ù„ÙŠØ© Ù„Ù„Ù€ parsing\"\"\"\n        start_time = datetime.now()\n\n        try:\n            # 1. Lexer\n            input_stream = InputStream(sql_code)\n            lexer = self.BaseLexer(input_stream)\n            token_stream = CommonTokenStream(lexer)\n\n            # Ø¹Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø²\n            token_stream.fill()\n            token_count = len(token_stream.tokens)\n\n            # 2. Parser\n            parser = self.SQLParser(token_stream)\n\n            # Error listener\n            error_listener = SQLErrorListener()\n            parser.removeErrorListeners()\n            parser.addErrorListener(error_listener)\n\n            # 3. Parse\n            tree = parser.sqlScript()\n\n            end_time = datetime.now()\n            parsing_time = (end_time - start_time).total_seconds() * 1000\n\n            # Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n            return {\n                'filename': filename,\n                'success': len(error_listener.errors) == 0,\n                'errors': error_listener.errors,\n                'parsing_time': parsing_time,\n                'tree_depth': self._get_tree_depth(tree),\n                'node_count': self._count_nodes(tree),\n                'token_count': token_count,\n                'tree': tree\n            }\n\n        except Exception as e:\n            end_time = datetime.now()\n            parsing_time = (end_time - start_time).total_seconds() * 1000\n\n            return {\n                'filename': filename,\n                'success': False,\n                'errors': [{'line': 0, 'column': 0, 'message': str(e), 'symbol': ''}],\n                'parsing_time': parsing_time,\n                'tree_depth': 0,\n                'node_count': 0,\n                'token_count': 0,\n                'tree': None\n            }\n\n    def _get_tree_depth(self, node, depth=0):\n        \"\"\"Ø­Ø³Ø§Ø¨ Ø¹Ù…Ù‚ Parse Tree\"\"\"\n        if node is None or node.getChildCount() == 0:\n            return depth\n        max_depth = depth\n        for i in range(node.getChildCount()):\n            child = node.getChild(i)\n            child_depth = self._get_tree_depth(child, depth + 1)\n            max_depth = max(max_depth, child_depth)\n        return max_depth\n\n    def _count_nodes(self, node):\n        \"\"\"Ø¹Ø¯ Ø§Ù„Ø¹Ù‚Ø¯ ÙÙŠ Parse Tree\"\"\"\n        if node is None: return 0\n        count = 1\n        for i in range(node.getChildCount()):\n            child = node.getChild(i)\n            count += self._count_nodes(child)\n        return count\n\n    def _print_tree_preview(self, node, rule_names, indent=0, max_lines=20):\n        \"\"\"Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ø§ÙŠÙ†Ø© Ù„Ù„Ø´Ø¬Ø±Ø© Ø¨Ø´ÙƒÙ„ Ù†ØµÙŠ\"\"\"\n        if node is None or max_lines <= 0:\n            return 0\n        \n        line_count = 1\n        prefix = \"  \" * indent\n        \n        if isinstance(node, TerminalNode):\n            print(f\"{prefix}TOKEN: {node.getText()}\")\n        else:\n            rule_index = node.getRuleIndex()\n            rule_name = rule_names[rule_index] if rule_index < len(rule_names) else str(rule_index)\n            print(f\"{prefix}RULE: {rule_name}\")\n            \n            for i in range(node.getChildCount()):\n                if line_count >= max_lines:\n                    if i == 0: print(f\"{prefix}  ...\")\n                    break\n                added_lines = self._print_tree_preview(node.getChild(i), rule_names, indent + 1, max_lines - line_count)\n                line_count += added_lines\n                \n        return line_count\n\n    def print_summary(self):\n        \"\"\"Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ğŸ“Š FINAL TEST SUMMARY\")\n        print(\"=\" * 80)\n\n        summary = self.results['summary']\n\n        print(f\"\\nğŸ“ Files Tested: {summary['total_files']}\")\n        print(f\"âœ… Passed: {summary['passed_files']}\")\n        print(f\"âŒ Failed: {summary['failed_files']}\")\n\n        if summary['total_files'] > 0:\n            success_rate = (summary['passed_files'] / summary['total_files']) * 100\n            print(f\"ğŸ“ˆ Success Rate: {success_rate:.1f}%\")\n\n        if summary['failed_files'] > 0:\n            print(f\"âš ï¸  Total Errors: {summary['total_errors']}\")\n\n        # ØªÙØ§ØµÙŠÙ„ ÙƒÙ„ Ù…Ù„Ù\n        print(f\"\\n{'=' * 80}\")\n        print(\"ğŸ“‹ Detailed Results:\")\n        print(f\"{'=' * 80}\")\n\n        for result in self.results['test_files']:\n            status = \"âœ… PASS\" if result['success'] else \"âŒ FAIL\"\n            errors = f\" ({len(result['errors'])} errors)\" if not result['success'] else \"\"\n            print(f\"{status:10} | {result['filename']:30} | {result['parsing_time']:8.2f}ms{errors}\")\n\n        # Ø£ÙØ¶Ù„ ÙˆØ£Ø³ÙˆØ£ Ø£Ø¯Ø§Ø¡\n        if self.results['test_files']:\n            print(f\"\\n{'=' * 80}\")\n            print(\"â±ï¸  Performance:\")\n            print(f\"{'=' * 80}\")\n\n            times = [r['parsing_time'] for r in self.results['test_files']]\n            avg_time = sum(times) / len(times)\n            max_time = max(times)\n            min_time = min(times)\n\n            print(f\"Average Parsing Time: {avg_time:.2f}ms\")\n            print(f\"Fastest: {min_time:.2f}ms\")\n            print(f\"Slowest: {max_time:.2f}ms\")\n\n        print(f\"\\n{'=' * 80}\\n\")\n\n    def save_report(self, filename='test_report.json'):\n        \"\"\"Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± JSON\"\"\"\n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(self.results, f, indent=2, ensure_ascii=False)\n        print(f\"ğŸ’¾ Report saved to: {filename}\")\n\n    def test_specific_queries(self):\n        \"\"\"Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ø­Ø¯Ø¯Ø©\"\"\"\n        print(\"\\n\" + \"=\" * 80)\n        print(\"ğŸ¯ SPECIFIC QUERY TESTS\")\n        print(\"=\" * 80)\n\n        test_cases = [\n            # Test 1: Simple SELECT\n            (\"SELECT * FROM Users;\", \"Simple SELECT\"),\n\n            # Test 2: JOIN\n            (\"SELECT u.Name, o.Total FROM Users u INNER JOIN Orders o ON u.ID = o.UserID;\",\n             \"INNER JOIN\"),\n\n            # Test 3: Subquery\n            (\"SELECT * FROM (SELECT ID FROM Users) AS T;\",\n             \"Subquery\"),\n\n            # Test 4: CASE\n            (\"SELECT CASE WHEN x = 1 THEN 'A' ELSE 'B' END FROM T;\",\n             \"CASE expression\"),\n\n            # Test 5: Multiple statements\n            (\"\"\"\n             CREATE TABLE T\n             (\n                 ID INT\n             );\n             INSERT INTO T\n             VALUES (1);\n             SELECT *\n             FROM T;\n             \"\"\", \"Multiple statements\"),\n\n            # Test 6: DECLARE and SET\n            (\"\"\"\n                DECLARE @x INT;\n                SET @x = 10;\n                SELECT @x;\n            \"\"\", \"Variables\"),\n\n            # Test 7: IF statement\n            (\"\"\"\n                IF NOT EXISTS (SELECT 1 FROM T)\n                BEGIN\n                    INSERT INTO T VALUES (1);\n                END\n            \"\"\", \"IF NOT EXISTS\"),\n\n            # Test 8: TRY-CATCH\n            (\"\"\"\n                BEGIN TRY\n                    SELECT 1/0;\n                END TRY\n                BEGIN CATCH\n                    SELECT ERROR_MESSAGE();\n                END CATCH\n            \"\"\", \"TRY-CATCH\"),\n        ]\n\n        passed = 0\n        failed = 0\n\n        for i, (sql, desc) in enumerate(test_cases, 1):\n            print(f\"\\nTest {i}: {desc}\")\n            print(\"-\" * 80)\n\n            result = self._parse(sql.strip(), f\"test_{i}.sql\")\nkeywordAsIdentifier\n    // Core SQL\n    : SELECT\n    | FROM\n    | WHERE\n    | INSERT\n    | INTO\n    | UPDATE\n    | DELETE\n    | CREATE\n    | ALTER\n    | TABLE\n    | DROP\n    | ADD\n    | VALUES\n    | SET\n\n    // Joins\n    | JOIN\n    | INNER\n    | LEFT\n    | RIGHT\n    | FULL\n    | OUTER\n    | ON\n\n    // Logical\n    | AND\n    | OR\n    | NOT\n    | EXISTS\n    | IN\n    | IS\n    | LIKE\n    | BETWEEN\n\n    // Control Flow\n    | IF\n    | BEGIN\n    | END\n    | TRY\n    | CATCH\n    | CASE\n    | WHEN\n    | THEN\n    | ELSE\n    | WITH\n\n    // Grouping & Ordering\n    | GROUP\n    | ORDER\n    | BY\n    | HAVING\n    | ASC\n    | DESC\n    | DISTINCT\n    | ALL\n    | UNION\n    | TOP\n    | PERCENT\n\n    // Constraints & Schema\n    | PRIMARY\n    | KEY\n    | CONSTRAINT\n    | COLUMN\n    | DEFAULT\n    | CLUSTERED\n    | NONCLUSTERED\n\n    // Data Types\n    | INT\n    | NVARCHAR\n    | VARCHAR\n    | CHAR\n    | BIGINT\n    | FLOAT\n    | DATETIME\n    | DATE\n    | DECIMAL\n    | BIT\n    | BINARY\n\n    // Functions\n    | MAX\n    | AVG\n    | COUNT\n    | LEN\n    | UPPER\n    | GETDATE\n    | VERSION\n    | VALUE\n\n    // Cursor & Execution\n    | DECLARE\n    | EXEC\n    | GO\n    | CURSOR\n    | FOR\n    | OPEN\n    | FETCH\n    | NEXT\n    | CLOSE\n    | DEALLOCATE\n    | SP_EXECUTESQL\n\n    // NULL handling\n    | NULL\n    ;\n\n            if result['success']:\n                print(f\"âœ… PASS - {result['parsing_time']:.2f}ms\")\n                # Ø¹Ø±Ø¶ Ø§Ù„Ø´Ø¬Ø±Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø©\n                self._print_tree_preview(result['tree'], self.SQLParser.ruleNames, indent=1, max_lines=10)\n                passed += 1\n            else:\n                print(f\"âŒ FAIL - {len(result['errors'])} errors\")\n                for err in result['errors'][:2]:\n                    print(f\"   Line {err['line']}: {err['message'][:80]}\")\n                failed += 1\n\n        print(f\"\\n{'=' * 80}\")\n        print(f\"Specific Tests Summary: {passed}/{len(test_cases)} passed\")\n        print(f\"{'=' * 80}\\n\")\n\n\ndef main():\n    \"\"\"Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\"\"\"\n    print(\"=\" * 80)\n    print(\"         COMPREHENSIVE SQL PARSER TEST SUITE\")\n    print(\"=\" * 80)\n    print()\n\n    # Ø¥Ù†Ø´Ø§Ø¡ Test Suite\n    suite = ComprehensiveTestSuite()\n\n    # Ù‚Ø§Ø¦Ù…Ø© Ù…Ù„ÙØ§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n    test_files = [\n        ('sqlInput.txt', 'Main test file - Complex DDL/DML'),\n        ('testing.sql', 'Same as sqlInput - Duplicate test'),\n        ('train.sql', 'Training queries - Various SQL patterns'),\n        ('train2.sql', 'Advanced training - Constraints & JOINs'),\n    ]\n\n    # Ø§Ø®ØªØ¨Ø§Ø± ÙƒÙ„ Ù…Ù„Ù\n    print(\"ğŸš€ TESTING ALL SQL FILES\")\n    print(\"=\" * 80)\n    print()\n\n    for filepath, description in test_files:\n        if Path(filepath).exists():\n            suite.test_file(filepath, description)\n        else:\n            print(f\"âš ï¸  Skipping {filepath} - File not found\\n\")\n\n    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ø­Ø¯Ø¯Ø©\n    suite.test_specific_queries()\n\n    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ\n    suite.print_summary()\n\n    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±\n    suite.save_report('parser_test_report.json')\n\n    print(\"\\nâœ… All tests completed!\")\n    print(\"=\" * 80)\n\n\nif __name__ == '__main__':\n    main()\n"
        }
    ]
}